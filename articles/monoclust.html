<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Monothetic Clustering 'monoClust' Package • monoClust</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Monothetic Clustering 'monoClust' Package">
<meta property="og:description" content="monoClust">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">monoClust</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.2.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/monoclust.html">Monothetic Clustering 'monoClust' Package</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/vinhtantran/monoClust/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="monoclust_files/accessible-code-block-0.0.1/empty-anchor.js"></script><link href="monoclust_files/anchor-sections-1.0/anchor-sections.css" rel="stylesheet">
<script src="monoclust_files/anchor-sections-1.0/anchor-sections.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Monothetic Clustering ‘monoClust’ Package</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/vinhtantran/monoClust/blob/master/vignettes/monoclust.Rmd"><code>vignettes/monoclust.Rmd</code></a></small>
      <div class="hidden name"><code>monoclust.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>Cluster analysis (or clustering) attempts to group observations into clusters so that the observations within a cluster are similar to each other while different from those in other clusters. It is often used when dealing with the question of discovering structure in data where no known group labels exist or when there might be some question about whether the data contain groups that correspond to a measured grouping variable. Therefore, cluster analysis is considered a type of unsupervised learning. It is used in many fields including statistics, machine learning, and image analysis, to name just a few. For a general introduction to cluster analysis, see Everitt and Hothorn (2011, Chapter 6).</p>
<p>Commonly used clustering methods are <span class="math inline">\(k\)</span>-means (MacQueen, 1967) and Ward’s hierarchical clustering (Murtagh and Legendre, 2014; Ward, 1963), which are both implemented in functions <code>kmeans</code> and <code>hclust</code>, respectively, in the <strong>stats</strong> package in R (R Core Team, 2019). They belong to a group of methods called <em>polythetic clustering</em> (MacNaughton-Smith et al., 1964) which use combined information of variables to partition data and generate groups of observations that are similar on average. Monothetic cluster analysis (Chavent, 1998; Piccarreta and Billari, 2007; Sneath and Sokal, 1973), on the other hand, is a clustering algorithm that provides a hierarchical, recursive partitioning of multivariate responses based on binary decision rules that are built from individual response variables. It creates clusters that contain shared characteristics that are defined by these rules.</p>
<p>Given a clustering algorithm, the cluster analysis is heavily influenced by the choice of <span class="math inline">\(K\)</span>, the number of clusters. If <span class="math inline">\(K\)</span> is too small, it puts “different” observations together. On the other hand, if <span class="math inline">\(K\)</span> is too large, the algorithm might split observations into different clusters that share many characteristics or are similar. Therefore, picking a sufficient, or “correct”, <span class="math inline">\(K\)</span> is critical for any clustering algorithm. A survey of some techniques for estimating the number of clusters has been done by Milligan and Cooper (1985). The R package <strong>NbClust</strong> (Charrad et al., 2014) is dedicated to implementations of those techniques. However, none of them were designed to work with monothetic clustering or take advantage of its unique characteristics, where binary splits generate rules for predicting new observations and each split is essentially a decision about whether to continue growing the tree or not. <span class="math inline">\(M\)</span>-fold cross-validation (a brief introduction can be seen in Hastie et al., 2016) and permutation-based hypothesis test at each split similar to those in Hothorn et al. (2006) are the two techniques that have been shown to work well in the classification and regression tree setting and we have adapted them to work with monothetic clustering.</p>
<p>Clustering data sets including circular variables, a type of variables measured in angles indicating the directions of an object or event (Fisher, 1993; Jammalamadaka and SenGupta, 2001) requires a different sets of statistical methods from conventional “linear” quantitative variables. An implementation of monothetic clustering modified to work on circular variables is discussed here. To assist in visualizing the resulting clusters and interpreting the shared features of clusters, a visualization of the results based on parallel coordinates plots (Inselberg and Dimsdale, 1987) are also implemented in the <strong>monoClust</strong> package. The package has been applied to the particle counts in föhn winds in Antarctica (<code>wind_sensit_2007</code> and <code>wind_sensit_2008</code> data sets from Šabacká et al., 2012).</p>
<!-- In an application of clustering to Arctic sea ice extent data comprising daily measurements from 1978 to present (Fetterer et al., 2018), we faced a challenge choosing one splitting variable among multiple equally qualified variables in monothetic clustering when applied to functional data. This happens because there are very similar observations in small intervals of time when the observations are smooth curves. A new clustering algorithm called Partitioning Using Local Subregions (PULS) that provides a method of clustering functional data using subregion information (described in detail in Chapter \ref{CH:functional}) is implemented in the \proglang{R} package \pkg{PULS}. It is designed to complement the \pkg{fda} and \pkg{fda.usc} packages \parencite{Febrero-Bande2012, fda2018} in clustering functional data objects. -->
</div>
<div id="monothetic-clustering" class="section level2">
<h2 class="hasAnchor">
<a href="#monothetic-clustering" class="anchor"></a>Monothetic Clustering</h2>
<p>Let <span class="math inline">\(y_{iq}\)</span> be the <span class="math inline">\(i^\mathrm{th}\)</span> observation (<span class="math inline">\(i = 1, \ldots, n\)</span>, the number of observations or sample size) on variable <span class="math inline">\(q\)</span> (<span class="math inline">\(q = 1, \ldots, Q\)</span>, the number of response variables) in a data set. In cluster analysis, <span class="math inline">\(Q\)</span> variables are considered “response” variables and the interest is in exploring potential groups in these responses. Occasionally other information in the data set is withheld from clustering to be able to understand clusters found based on the <span class="math inline">\(Q\)</span> variables used in clustering. Clustering algorithms then attempt to partition the <span class="math inline">\(n\)</span> observations into mutually exclusive clusters <span class="math inline">\(C_1, C_2, \ldots, C_K\)</span> in <span class="math inline">\(\Omega\)</span> where <span class="math inline">\(K\)</span> is the number of clusters, so that the observations within a cluster are “close” to each other and “far away” from those in other clusters.</p>
<p>Inspired by regression trees (Breiman et al., 1984) and the <strong>rpart</strong> package (Therneau and Atkinson, 2018), the monothetic clustering algorithm searches for splits from each response variable that provide the best split of the multivariate responses in terms of a global criterion called inertia. To run successfully on a data set, the <code>MonoClust</code> function of the <strong>monoClust</strong> package only has one required argument, which is the data set name in the <code>toclust</code> option. By default, <code>MonoClust</code> will be performed by first calculating the squared Euclidean distance matrix between observations. The calculation of the distance matrix is very important to the algorithm because <em>inertia</em>, a within-cluster measure of variability when Euclidean distance is used, is calculated by \begin{equation} I(C_k) =\sum_{i \in C_k} d^2_{euc}(\mathbf{y_i}, \overline{y}_{C_k}), \end{equation} where <span class="math inline">\(\overline{y}_{C_k}\)</span> is the mean of all observations in cluster <span class="math inline">\(C_k\)</span>. This formula has been proved to be equivalent to the scaled sum of squared Euclidean distances among all observations in a cluster (James et al., 2013, p 388), \begin{equation} I(C_k) = \frac{1}{n_k} \sum_{(i, j) \in C_k, i &gt; j} d^2_{euc}(\mathbf{y_i},\mathbf{y_j}). \end{equation}</p>
<p>A binary split, <span class="math inline">\(s(C_k)\)</span>, on a cluster <span class="math inline">\(C_k\)</span> divides its observations into two smaller clusters <span class="math inline">\(C_{kL}\)</span> and <span class="math inline">\(C_{kR}\)</span>. The inertia decrease between before and after the partition is defined as \begin{equation} \Delta (s, C_k) = I(C_k) - I(C_{kL}) - I(C_{kR}), \end{equation} and the best split, <span class="math inline">\(s^*(C_k)\)</span>, is the split that maximizes this decrease in inertia, \begin{equation} s^*(C_k) = \arg \max_s \Delta(s, C_k). \end{equation} The same algorithm is then recursively applied to each sub-partition, recording splitting rules on its way until it reaches the stopping criteria, which can be set in <code>MonoClust</code> by at least one of these arguments:</p>
<ul>
<li>
<code>nclusters</code>: the pre-defined number of resulting clusters;</li>
<li>
<code>minsplit</code>: the minimum number of observations that must exist in a node in order for a split to be attempted (default is 5);</li>
<li>
<code>minbucket</code>: the minimum number of observations allowed in a terminal leaf (default is <code>minsplit</code>/3).</li>
</ul>
<p>As a very simple example, monothetic clustering of the <em>ruspini</em> data set (Ruspini, 1970) available in the <strong>cluster</strong> package (Maechler et al., 2018) with 4 clusters can be performed as follows:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://vinhtantran.github.io/monoClust/">monoClust</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://svn.r-project.org/R-packages/trunk/cluster">cluster</a></span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">ruspini</span><span class="op">)</span>
<span class="va">ruspini4c</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/MonoClust.html">MonoClust</a></span><span class="op">(</span><span class="va">ruspini</span>, nclusters <span class="op">=</span> <span class="fl">4</span><span class="op">)</span>
<span class="va">ruspini4c</span>
<span class="co">#&gt; n = 75 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Node) Split, N, Cluster Inertia, Proportion Inertia Explained,  </span>
<span class="co">#&gt;       * denotes terminal node</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; 1) root 75 244373.900 0.6344215   </span>
<span class="co">#&gt;   2) y &lt; 91 35  43328.460 0.9472896   </span>
<span class="co">#&gt;     4) x &lt; 37 20   3689.500   *</span>
<span class="co">#&gt;     5) x &gt;= 37 15   1456.533   *</span>
<span class="co">#&gt;   3) y &gt;= 91 40  46009.380 0.7910436   </span>
<span class="co">#&gt;     6) x &lt; 63.5 23   3176.783   *</span>
<span class="co">#&gt;     7) x &gt;= 63.5 17   4558.235   *</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Note: One or more of the splits chosen had an alternative split that reduced inertia by the same amount. See "alt" column of "frame" object for details.</span></code></pre></div>
<p>The output (<code>print.MonoClust</code>) lists each split on one line together with the splitting rule as well as its inertia and is displayed with the hierarchical structure so the parent–child relationships between nodes can be easily seen. This function defines a <code>MonoClust</code> object to store the cluster solution with some useful components:</p>
<ul>
<li>
<code>frame</code>: a partitioning tracking table in the form of a <code>data.frame</code>;</li>
<li>
<code>membership</code>: a vector of numerical cluster identification (incremented by new nodes) that observations belong to; and</li>
<li>
<code>medoids</code>: the observation indices that are considered as representatives for the clusters (Kaufman and Rousseeuw, 1990), estimated as the observations that have minimum total distance to all observations in their clusters.</li>
</ul>
<p>Another visualization of the clustering results is the splitting rule tree created by the <code>plot.MonoClust</code> function of the <code>MonoClust</code> object.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ruspini4c</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="monoclust_files/figure-html/ruspinip-1.png" alt="Binary partitioning tree with three splits, four clusters for *ruspini* data." width="576"><p class="caption">
Binary partitioning tree with three splits, four clusters for <em>ruspini</em> data.
</p>
</div>
<p>The initial development of <strong>MonoClust</strong> was based on <strong>rpart</strong>. However, we steered away to develop our own algorithms to significantly enhance the structure to implement the various methods.</p>
</div>
<div id="testing-at-each-split-to-decide-the-number-of-clusters" class="section level2">
<h2 class="hasAnchor">
<a href="#testing-at-each-split-to-decide-the-number-of-clusters" class="anchor"></a>Testing at Each Split to Decide the Number of Clusters</h2>
<p>Deciding on the number of clusters to report and interpret is an important part of cluster analysis. Among many metrics mentioned in Milligan and Cooper (1985) and Hardy (1996), Caliński and Harabasz (CH)’s pseudo-<span class="math inline">\(F\)</span> (Caliński and Harabasz, 1974) is among the metrics that have typically good or even the best performance in the Milligan and Cooper (1985)’s simulation studies on selecting the optimal number of clusters. Additionally, average silhouette width (AW), a measure of how “comfortable” observations are in their clusters they reside in, has also been suggested to select an appropriate number of clusters (Rousseeuw, 1987). One limitation of both criteria is that they are unable to select a single cluster solution because their formula require at least two clusters to calculate the criteria. We have proposed two methods that can assist select the number of clusters in monothetic clustering. One is inspired by regression tree methods for pruning regression and classification trees, which is an adaption of <span class="math inline">\(M\)</span>-fold cross-validation technique. Another one is inspired by conditional inference trees (Hothorn et al., 2006). It is a formal hypothesis test at a split to determine if it should be performed using two different test statistics. Finally, we suggested a hybrid method that uses the hypothesis test with CH’s <span class="math inline">\(F\)</span> statistic at the first split and then uses the original CH’s <span class="math inline">\(F\)</span> for the further splits if the test suggests that there should be at least two clusters.</p>
<p>The <span class="math inline">\(M\)</span>-fold cross-validation randomly partitions data into <span class="math inline">\(M\)</span> subsets with equal (or close to equal) sizes. <span class="math inline">\(M - 1\)</span> subsets are used as the training data set to create a tree with a desired number of leaves and the other subset is used as validation data set to evaluate the predictive performance of the trained tree. The process repeats for each subset as the validating set (<span class="math inline">\(m = 1, \ldots, M\)</span>) and the mean squared difference, \begin{equation} MSE_m=\frac{1}{n_m} \sum_{q=1}^Q\sum_{i \in m} d^2_{euc}(y_{iq}, \hat{y}_{(-i)q}), \end{equation} is calculated, where <span class="math inline">\(\hat{y}_{(-i)q}\)</span> is the cluster mean on the variable <span class="math inline">\(q\)</span> of the cluster created by the training data where the observed value, <span class="math inline">\(y_{iq}\)</span>, of the validation data set will fall into, and <span class="math inline">\(d^2_{euc}(y_{iq}, \hat{y}_{(-i)q})\)</span> is the squared Euclidean distance (dissimilarity) between two observations at variable <span class="math inline">\(q\)</span>. This process is repeated for the <span class="math inline">\(M\)</span> subsets of the data set and the average of these test errors is the cross-validation-based estimate of the mean squared error of predicting a new observation, \begin{equation} CV_K = \overline{MSE} = \frac{1}{M} \sum_{m=1}^M MSE_m. \end{equation}</p>
<p>The purpose of the cross-validation is to find a cluster solution that achieves the “best” prediction error for new observations. There are several ways one can decide from the output of <code>MonoClust</code>. A naive approach is to pick the solution that has the smallest <span class="math inline">\(CV_K\)</span> (<em>minCV</em> rule). However, in many cases, it can result in a very high number of clusters if the error rate keeps decreasing even though there is often a small change after a few large drops. To avoid this problem, Breiman et al. (1984) suggested picking the solution that is simplest within 1 or 2 standard errors (SE) from the minimum error estimate (<em>CV1SE</em> or <em>CV2SE</em> rules), with the standard error is defined as <span class="math display">\[SE(\overline{MSE}) = \sqrt{\frac{1}{M} \sum_{m=1}^M (MSE_m - \overline{MSE})}.\]</span> The function <code>cv.test</code> with the data set and two arguments <code>minnodes</code> and <code>maxnodes</code> defining the range of nodes to test on will apply <code>MonoClust</code> and calculate both <span class="math inline">\(\overline{MSE}\)</span> (which is named MSE in the output) and its standard error (named Std. Dev.).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">12345</span><span class="op">)</span>
<span class="va">cp.table</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.test.html">cv.test</a></span><span class="op">(</span><span class="va">ruspini</span>, fold <span class="op">=</span> <span class="fl">5</span>, minnodes <span class="op">=</span> <span class="fl">1</span>, maxnodes <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>
<span class="va">cp.table</span>
<span class="co">#&gt; 5-fold Cross-validation on a MonoClust object </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; # A tibble: 10 x 3</span>
<span class="co">#&gt;    ncluster    MSE `Std. Dev.`</span>
<span class="co">#&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;</span>
<span class="co">#&gt;  1        1 51192.       4436.</span>
<span class="co">#&gt;  2        2 19170.       2996.</span>
<span class="co">#&gt;  3        3 14209.       4395.</span>
<span class="co">#&gt;  4        4  4572.       2905.</span>
<span class="co">#&gt;  5        5  4083.       2598.</span>
<span class="co">#&gt;  6        6  4048.       2540.</span>
<span class="co">#&gt;  7        7  3788.       2554.</span>
<span class="co">#&gt;  8        8  3696.       2603.</span>
<span class="co">#&gt;  9        9  3598.       2681.</span>
<span class="co">#&gt; 10       10  3500.       2689.</span></code></pre></div>
<p>A plot with error bars for one standard error, similar to Figure @ref(fig:cvv), can be made from the output table using standard plotting functions to assist in assessing these results.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>
<span class="fu"><a href="../reference/ggcv.html">ggcv</a></span><span class="op">(</span><span class="va">cp.table</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">lower1SD</span><span class="op">)</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_hline</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>yintercept <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">upper1SD</span><span class="op">)</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">ncluster</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span>, y <span class="op">=</span> <span class="va">MSE</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">ncluster</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span>, y <span class="op">=</span> <span class="va">MSE</span><span class="op">[</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">5</span>, shape <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="monoclust_files/figure-html/cvv-1.png" alt="The choice of clusters for Ruspini data made by 10-fold CV where *minCV* selects 10 clusters and *1SE* selects 4. The error bars are the $\overline{MSE} \pm 1SE$ and the choice of 4 clusters, the simplest solution within 1 standard error of the minimum error estimate (the dashed lines coincide with the bar at 10 clusters) is highlighted with a $\times$." width="576"><p class="caption">
The choice of clusters for Ruspini data made by 10-fold CV where <em>minCV</em> selects 10 clusters and <em>1SE</em> selects 4. The error bars are the <span class="math inline">\(\overline{MSE} \pm 1SE\)</span> and the choice of 4 clusters, the simplest solution within 1 standard error of the minimum error estimate (the dashed lines coincide with the bar at 10 clusters) is highlighted with a <span class="math inline">\(\times\)</span>.
</p>
</div>
<p>Another approach involves doing a formal hypothesis test at each split on the tree and using the <span class="math inline">\(p\)</span>-values to decide on how many clusters should be used. This approach has been used in the context of conditional inference trees by Hothorn et al. (2006) although with a different test statistic and purpose. In that situation the test is used to test a null hypothesis of independence between the response and a selected predictor. For cluster analysis, at any cluster (or leaf on the decision tree), whether it will be partitioned or not is the result of a hypothesis test in which the pair of hypotheses can be abstractly stated as</p>
<p><span class="math inline">\(H_0:\)</span> The two new clusters are identical to each other, and</p>
<p><span class="math inline">\(H_A:\)</span> The two new clusters are different from each other.</p>
<p>To allow applications with any dissimilarity measure, a nonparametric method based on permutation is used. Anderson (2001) developed a multivariate nonparametric testing approach called <em>perMANOVA</em> that involves calculating the pseudo-<span class="math inline">\(F\)</span>-ratio directly from any symmetric distance or dissimilarity matrix where the sum of squares are, in turn, calculated from the dissimilarities. The <span class="math inline">\(p\)</span>-value can then be calculated by tracking the pseudo-<span class="math inline">\(F\)</span> across permutations and comparing the results to the observed result and is available in the <strong>vegan</strong> package (Oksanen et al., 2019) in R. We considered two approaches for generating the permutation distribution under the previous null:</p>
<ol style="list-style-type: decimal">
<li>Shuffle the observations between two proposed clusters. The pseudo-<span class="math inline">\(F\)</span>’s calculated from the shuffles create the reference distribution to find the <span class="math inline">\(p\)</span>-value. Because the splitting variable that was chosen is already the best in terms of reduction of inertia, that variable is withheld from the distance matrix used in the permutation test. This method can be done with <code>method = "sw"</code> (default value) in the <code>perm.test</code> function.</li>
<li>Shuffle the values of the splitting variables while keeping other variables fixed to create a new data set, then the average silhouette width (Kaufman and Rousseeuw, 1990) is used as the measure of separation between the two new clusters and is calculated to create the reference distribution. Specifying <code>method = "rl"</code> in <code>perm.test</code> will run this method.</li>
<li>Similar to the previous method but pseudo-<span class="math inline">\(F\)</span> (as in the first approach) is used as the test statistic instead of the average silhouette width. This approach corresponds to <code>method = "rn"</code>.</li>
</ol>
<p>Applying the <code>perm.test</code> function to a <code>MonoClust</code> object will add permutation-based <span class="math inline">\(p\)</span>-values to the output of both <code>print</code> and <code>plot</code>. Users can specify the number of permutations with the <code>rep =</code> argument. An example of applying the cluster shuffling approach to the <em>ruspini</em> data set follows and the tree output is in Figure @ref(fig:hyptestv). Note that the Bonferroni-adjusted <span class="math inline">\(p\)</span>-values are used to account for the multiple hypothesis tests required when going deeper into the tree. The number of tests for the adjustment is based on the number of tests previously performed to get to a candidate split and the maximum value of a <span class="math inline">\(p\)</span>-value is always 1. A similar adjustment has been used in conditional inference trees and was also implemented in its accompanied <strong>party</strong> package (Hothorn et al., 2006).</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ruspini6c</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/MonoClust.html">MonoClust</a></span><span class="op">(</span><span class="va">ruspini</span>, nclusters <span class="op">=</span> <span class="fl">6</span><span class="op">)</span>
<span class="va">ruspini6c.pvalue</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/perm.test.html">perm.test</a></span><span class="op">(</span><span class="va">ruspini6c</span>, data <span class="op">=</span> <span class="va">ruspini</span>, method <span class="op">=</span> <span class="st">"sw"</span>, rep <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ruspini6c.pvalue</span>, branch <span class="op">=</span> <span class="fl">1</span>, uniform <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="monoclust_files/figure-html/hyptestv-1.png" alt="Binary partitioning tree with five splits, six clusters, but one split should be pruned based on its p-value of 0.8." width="960"><p class="caption">
Binary partitioning tree with five splits, six clusters, but one split should be pruned based on its p-value of 0.8.
</p>
</div>
</div>
<div id="clustering-on-circular-data" class="section level2">
<h2 class="hasAnchor">
<a href="#clustering-on-circular-data" class="anchor"></a>Clustering on Circular Data</h2>
<p>In many applications, a variable can be measured in angles, indicating the directions of an object or event. Examples could be the times of day, aspects of the slope in mountainous terrain, directions of motion, or wind directions. Such variables are referred to as <em>circular variables</em> and are measured either in degrees or radians relative to a pre-chosen 0 degree position and meaning of a rotation direction. There are books dedicated to this topic (for example, Fisher, 1993; Jammalamadaka and SenGupta, 2001) that develop parametric models and analytic tools for circular variables. Here we demonstrate multivariate data analysis involving circular variables, such as visualization and clustering.</p>
<p>Cluster analysis depends on the choice of distance or dissimilarity between multivariate data points. A (dis)similarity measure that often comes up in the literature when dealing with mixed data types is Gower’s distance (Gower, 1971). It is a similarity measure among observations from various types of variables, such as quantitative, categorical, and binary, can be a reasonable alternative to Euclidean distance when working with “mixed” data.</p>
<p>Generally, the Gower’s dissimilarity in a simple form (without weights) for a data set with <span class="math inline">\(Q\)</span> variables is <span class="math display">\[d_{gow}(\mathbf{y_i},\mathbf{y_j}) = \frac{1}{Q} \sum_{q=1}^Q d_{gow}(y_{iq}, y_{jq}).\]</span> If <span class="math inline">\(q\)</span> is a linear quantitative variable, <span class="math display">\[d(y_{iq}, y_{jq}) = \frac{|y_{iq} - y_{jq}|}{\max_{i,j}|y_{iq} - y_{jq}|}.\]</span> It can also incorporate categorical variables, with <span class="math inline">\(d(y_{iq}, y_{jq})\)</span> equals to 0 if the two observations belong to the same category of <span class="math inline">\(q\)</span> and 1 otherwise. Details and examples can be seen in Everitt et al. (2011, Chapter 3). We extend the dissimilarity measure for a circular variable as <span class="math display">\[d(y_{iq},y_{jq}) = \frac{180 - \left| 180 - |y_{iq} - y_{jq}| \right|}{180},\]</span> where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are the angles in degree. If radians are used, the constant 180 degrees will be replaced by <span class="math inline">\(\pi\)</span>. This distance can be mixed with other Gower’s distances both for monothetic clustering and in other distance-based clustering algorithms.</p>
<p>We demonstrated an application of monothetic clustering to a data set from Šabacká et al. (2012). This data set is a part of a study on microorganisms carried in föhn winds at the Taylor Valley, an ice free area in the Antarctic continent. The examined subset of the data is during July 7–14, 2008, at the Bonney Riegel location with three variables: the existence of particles measured in 1 minute every 15 minutes (binary variable of presence or absence), average wind speed (m/s), and wind direction (degrees) recorded at a nearby meteorological station every 15 minutes. Wind direction is a circular variable in which winds blowing from the north to the south were chosen to be 0/360 degrees and winds blowing from the east to the west were chosen to be 90 degrees. <code>MonoClust</code> works on circular data by indicating the index or name of the circular variable (if there is more than one circular variable, a vector of them can be transferred) in the <code>cir.var</code> argument.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">wind_sensit_2008</span><span class="op">)</span>
<span class="co"># For the sake of speed in the example</span>
<span class="va">wind_reduced_2008</span> <span class="op">&lt;-</span> <span class="va">wind_sensit_2008</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample.int</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">wind_sensit_2008</span><span class="op">)</span>, <span class="fl">50</span><span class="op">)</span>, <span class="op">]</span>
<span class="va">sensit042008</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/MonoClust.html">MonoClust</a></span><span class="op">(</span><span class="va">wind_reduced_2008</span>, nclusters <span class="op">=</span> <span class="fl">4</span>, cir.var <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
<span class="co">#&gt; Warning in cluster::daisy(toclust[, -cir.var], metric = distmethod): binary</span>
<span class="co">#&gt; variable(s) 1 treated as interval scaled</span></code></pre></div>
<p>To perform monothetic clustering, a variable must generate a binary split. Because of special circular characteristics, a circle needs two cuts to create two separate arcs instead of one cut-point as in conventional linear variables. Therefore, the algorithm to search for the best split in a circular variable is actually done in two folds; first by fixing one cut value and then searching for the second cut. This process is repeated by changing the first cut until all possible pairs of cuts have been examined and the best two cuts are then picked based on the inertia. The splitting rule tree is also updated to add the second split value on the corner of the tree. After the first split on a circular variable, the arcs can be considered as two conventional quantitative variables and can be split further with only a single cut-point. Figure @ref(fig:sensit2008plot) shows the resulting four clusters created by applying monothetic clustering on the Antarctic data.</p>
<p>When clustering a data set that has at least one circular variable in it, visualizing the cluster results to detect the underlying characteristics of the clusters is very crucial. Scatterplots are not very helpful for circular data because of the characteristics of those variables. Dimension reduction can be performed using techniques like multi-dimensional scaling (Chapter 14, Hastie et al., 2016) or more recent techniques such as t-SNE (Hinton, 2008), but the details of the original variables are lost in these projections. Parallel Coordinates Plots (PCPs, Inselberg and Dimsdale, 1987), which can display the original values of all of the multivariate data by putting them on equally spaced vertical x-axes, are a good choice due to its simplicity and its capability to retain the proximity of the data points (Härdle and Simar, 2015, Chapter 1). A modified PCP, inspired by Will (2016), is also implemented in <strong>monoClust</strong> using <strong>ggplot2</strong> (Wickham, 2016). The circular variable is displayed as an ellipse with the options to rotate and/or change the order of appearance of variables to help facilitate the detection of underlying properties of the clusters. Figure @ref(fig:PCPellipsev) is the PCP of the Antarctic data with the cluster memberships colored and matched to the tree in Figure @ref(fig:sensit2008plot) with the following code. There are other display options that can be modified such as the transparency of lines, whether the circular variable is in degrees or radians, etc. (see the function documentation for details).</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="../reference/ggpcp.html">ggpcp</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">wind_reduced_2008</span>, 
      circ.var <span class="op">=</span> <span class="st">"WDIR"</span>,
      rotate <span class="op">=</span> <span class="va">pi</span> <span class="op">/</span> <span class="fl">4</span> <span class="op">+</span> <span class="fl">0.6</span>,
      order.appear <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"WDIR"</span>, <span class="st">"has.sensit"</span>, <span class="st">"WS"</span><span class="op">)</span>,
      clustering <span class="op">=</span> <span class="va">sensit042008</span><span class="op">$</span><span class="va">membership</span>, 
      medoids <span class="op">=</span> <span class="va">sensit042008</span><span class="op">$</span><span class="va">medoids</span>,
      alpha <span class="op">=</span> <span class="fl">0.5</span>,
      cluster.col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#e41a1c"</span>, <span class="st">"#377eb8"</span>, <span class="st">"#4daf4a"</span>, <span class="st">"#984ea3"</span><span class="op">)</span>,
      show.medoids <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="figure">
<img src="monoclust_files/figure-html/PCPellipsev-1.png" alt="PCP with the circular variable (*WDIR*) depicted as an ellipse. The geographical direction is noted and the ellipse is rotated to facilitate understanding of clusters." width="576"><p class="caption">
PCP with the circular variable (<em>WDIR</em>) depicted as an ellipse. The geographical direction is noted and the ellipse is rotated to facilitate understanding of clusters.
</p>
</div>
</div>
<div id="bibliography" class="section level2">
<h2 class="hasAnchor">
<a href="#bibliography" class="anchor"></a>Bibliography</h2>
<ul>
<li>Anderson, M. J. (2001). “A new method for non-parametric multivariate analysis of variance”. In: <em>Austral Ecology</em> 26.1, pp. 32{46. issn: 14429985. doi: <code>10.1111/j.1442-9993.2001.01070.pp.x</code>.</li>
<li>Breiman, L., J. Friedman, C. J. Stone, and R. Olshen (1984). <em>Classification and Regression Trees</em>. 1st ed. Chapman and Hall/CRC. isbn: 0412048418.</li>
<li>Caliński, T. and J Harabasz (1974). “A dendrite method for cluster analysis”. en. In: <em>Communications in Statistics</em> 3.1, pp. 1-27.</li>
<li>Charrad, M., N. Ghazzali, V. Boiteau, and A. Niknafs (2014). “NbClust: An R Package for Determining the”. In: <em>Journal of Statistical Software</em> 61.6.</li>
<li>Chavent, M. (1998). “A monothetic clustering method”. In: <em>Pattern Recognition Letters</em> 19.11, pp. 989{996. issn: 01678655. doi: <code>10.1016/S0167-8655(98)00087-7</code>.</li>
<li>Everitt, B. and T. Hothorn (2011). <em>An Introduction to Applied Multivariate Analysis with R</em>. 1st ed. Springer. isbn: 1441996494.</li>
<li>Everitt, B. S., S. Landau, M. Leese, and D. Stahl (2011). <em>Cluster Analysis</em>. 5th ed. Wiley, p. 346. isbn: 0470749911.</li>
<li>Fisher, N. I. (1993). <em>Statistical Analysis of Circular Data</em>. Cambridge: Cambridge University Press. isbn: 9780511564345. doi: <code>10.1017/CBO9780511564345</code>.</li>
<li>Gower, J. C. (1971). “A General Coefficient of Similarity and Some of Its Properties”. In: <em>Biometrics</em> 27.4, p. 857. issn: 0006341X. doi: <code>10.2307/2528823</code>.</li>
<li>Härdle , W. K. and L. Simar (2015). <em>Applied Multivariate Statistical Analysis</em>. Berlin, Heidelberg: Springer Berlin Heidelberg. isbn: 978-3-662-45170-0. doi: 10.1007/978-3-662-45171-7.</li>
<li>Hardy, A. (1996). On the number of clusters". In: Computational Statistics &amp; Data Analysis 23.1, pp. 83{96. issn: 0167-9473. doi: <code>10.1016/S0167-9473(96)00022-9</code>.</li>
<li>Hastie, T., R. Tibshirani, and J. Friedman (2016). <em>The Elements of Statistical Learning</em>. 2nd ed. Springer. isbn: 978-0387848570.</li>
<li>Hinton, G. (2008). “Visualizing Data using t-SNE”. In: <em>Journal of Machine Learning Research</em> 9.Nov, pp. 2579{2605. issn: 02545330. doi: <code>10.1007/s10479-011-0841-3</code>.</li>
<li>Hothorn, T., K. Hornik, and A. Zeileis (2006). “Unbiased Recursive Partitioning: A Conditional Inference Framework”. en. In: <em>Journal of Computational and Graphical Statistics</em> 15.3, pp. 651–674. issn: 1061-8600. doi: <code>10.1198/106186006X133933.33</code>
</li>
<li>Inselberg, A. and B. Dimsdale (1987). “Parallel Coordinates for Visualizing Multi-Dimensional Geometry”. In: <em>Computer Graphics 1987</em>. Tokyo: Springer Japan, pp. 25{44. doi: <code>10.1007/978-4-431-68057-4_3</code>.</li>
<li>James, G., D. Witten, T. Hastie, and R. Tibshirani (2013). <em>An Introduction to Statistical Learning: with Applications in R</em>. 1st. Springer, p. 426. isbn: 1461471370.</li>
<li>Jammalamadaka, S. R. and A. SenGupta (2001). <em>Topics in Circular Statistics</em>. Vol. 5. isbn: 9789812779267. doi: <code>10.1142/9789812779267</code>.</li>
<li>Kaufman, L. and P. J. Rousseeuw (1990). <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. 1st ed. Wiley-Interscience, p. 368. isbn: 978-0471735786.</li>
<li>MacNaughton-Smith, P., W. T. Williams, M. B. Dale, and L. G. Mockett (1964). “Dissimilarity Analysis: a new Technique of Hierarchical Sub-division”. In: <em>Nature</em> 202.4936, pp. 1034{1035. issn: 0028-0836. doi: <code>10.1038/2021034a0</code>.</li>
<li>MacQueen, J. (1967). <em>Some methods for classification and analysis of multivariate observations</em>. Berkeley, Calif.</li>
<li>Maechler, M., P. Rousseeuw, A. Struyf, M. Hubert, and K. Hornik (2018). <em>cluster: Cluster Analysis Basics and Extensions</em>. R package version 2.0.7-1 — For new features, see the ‘Changelog’ file (in the package source).</li>
<li>Milligan, G. W. and M. C. Cooper (1985). “An examination of procedures for determining the number of clusters in a data set”. In: <em>Psychometrika</em> 50.2, pp. 159–179. issn: 0033-3123. doi: <code>10.1007/BF02294245</code>.</li>
<li>Murtagh, F. and P. Legendre (2014). “Ward’s Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward’s Criterion?” In: <em>Journal of Classification</em> 31.3, pp. 274–295. issn: 0176-4268. doi: <code>10.1007/s00357-014-9161-z.34</code>
</li>
<li>Piccarreta, R. and F. C. Billari (2007). “Clustering work and family trajectories by using a divisive algorithm”. In: <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em> 170.4, pp. 1061–1078. issn: 0964-1998. doi: <code>10.1111/j.1467-985X.2007.00495.x.</code>
</li>
<li>R Core Team (2020). <em>R: A Language and Environment for Statistical Computing</em>. R Foundation for Statistical Computing. Vienna, Austria. url: <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</li>
<li>Rousseeuw, P. J. (1987). “Silhouettes: A graphical aid to the interpretation and validation of cluster analysis”. In: <em>Journal of Computational and Applied Mathematics</em> 20, pp. 53–65. issn: 03770427. doi: <code>10.1016/0377-0427(87)90125-7</code>.</li>
<li>Ruspini, E. H. (1970). “Numerical methods for fuzzy clustering”. In: <em>Information Sciences</em> 2.3, pp. 319–350. issn: 00200255. doi: <code>10.1016/S0020-0255(70)80056-1</code>.</li>
<li>Šabacká, M., J. C. Priscu, H. J. Basagic, A. G. Fountain, D. H. Wall, R. A. Virginia, and M. C. Greenwood (2012). “Aeolian ux of biotic and abiotic material in Taylor Valley, Antarctica”. In: <em>Geomorphology</em> 155-156, pp. 102–111. issn: 0169555X. doi: <code>10.1016/j.geomorph.2011.12.009.35</code>
</li>
<li>Sneath, P. H. A. and R. R. Sokal (1973). <em>Numerical taxonomy: the principles and practice of numerical classification</em>. W.H. Freeman, p. 573. isbn: 0716706970.</li>
<li>Therneau, T. and B. Atkinson (2018). <em>rpart: Recursive Partitioning and Regression Trees</em>. R package version 4.1-13.</li>
<li>Ward, J. H. (1963). “Hierarchical Grouping to Optimize an Objective Function”. In: <em>Journal of the American Statistical Association</em> 58.301, pp. 236–244. issn: 0162-1459. doi: <code>10.1080/01621459.1963.10500845</code>.</li>
<li>Wickham, H. (2016). <em>ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. isbn: 978-3-319-24277-4. url: <a href="https://ggplot2.tidyverse.org" class="uri">https://ggplot2.tidyverse.org</a>.</li>
<li>Will, G. (2016). “Visualizing and Clustering Data that Includes Circular Variables”. Writing Project. Montana State University.</li>
</ul>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Tan Tran, Brian McGuire, Mark Greenwood.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
